#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆä»£ç è´¨é‡æ£€æŸ¥å·¥å…·
å¿«é€Ÿè¯†åˆ«é¡¹ç›®ä¸­çš„å…³é”®æŠ€æœ¯å€ºåŠ¡é—®é¢˜
"""

import os
import re
from pathlib import Path
from collections import Counter, defaultdict

def get_python_files(project_root):
    """è·å–æ‰€æœ‰Pythonæ–‡ä»¶"""
    python_files = []
    for root, dirs, files in os.walk(project_root):
        # è·³è¿‡è™šæ‹Ÿç¯å¢ƒå’Œç¼“å­˜ç›®å½•
        dirs[:] = [d for d in dirs if d not in {'.git', '__pycache__', '.pytest_cache', 'venv', 'env', '.venv'}]
        
        for file in files:
            if file.endswith('.py'):
                python_files.append(Path(root) / file)
    
    return python_files

def read_file_safe(file_path):
    """å®‰å…¨è¯»å–æ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except (UnicodeDecodeError, FileNotFoundError):
        try:
            with open(file_path, 'r', encoding='gbk') as f:
                return f.read()
        except Exception:
            return None

def analyze_hardcoded_imports(project_root):
    """åˆ†æç¡¬ç¼–ç å¯¼å…¥"""
    issues = []
    patterns = [
        r'from\s+src\.',
        r'import\s+src\.',
        r'sys\.path\.append',
        r'os\.path\.dirname.*__file__'
    ]
    
    for file_path in get_python_files(project_root):
        content = read_file_safe(file_path)
        if not content:
            continue
        
        lines = content.split('\n')
        for line_num, line in enumerate(lines, 1):
            for pattern in patterns:
                if re.search(pattern, line):
                    issues.append({
                        'file': str(file_path.relative_to(Path(project_root))),
                        'line': line_num,
                        'type': 'hardcoded_import',
                        'severity': 'major',
                        'description': f'ç¡¬ç¼–ç å¯¼å…¥: {line.strip()}',
                        'code': line.strip()
                    })
    
    return issues

def analyze_code_complexity(project_root):
    """åˆ†æä»£ç å¤æ‚åº¦"""
    issues = []
    file_stats = {}
    
    for file_path in get_python_files(project_root):
        content = read_file_safe(file_path)
        if not content:
            continue
        
        lines = content.split('\n')
        non_empty_lines = [line for line in lines if line.strip() and not line.strip().startswith('#')]
        
        # ç®€å•çš„å¤æ‚åº¦æŒ‡æ ‡
        function_count = len([line for line in lines if re.match(r'^\s*def\s+', line)])
        class_count = len([line for line in lines if re.match(r'^\s*class\s+', line)])
        
        # è®¡ç®—æœ€å¤§åµŒå¥—æ·±åº¦
        max_indent = 0
        for line in lines:
            if line.strip():
                indent = len(line) - len(line.lstrip())
                max_indent = max(max_indent, indent // 4)  # å‡è®¾4ç©ºæ ¼ç¼©è¿›
        
        relative_path = str(file_path.relative_to(Path(project_root)))
        file_stats[relative_path] = {
            'lines_of_code': len(non_empty_lines),
            'function_count': function_count,
            'class_count': class_count,
            'max_nesting': max_indent
        }
        
        # æ£€æŸ¥å¤æ‚åº¦é˜ˆå€¼
        if len(non_empty_lines) > 500:
            issues.append({
                'file': relative_path,
                'line': 1,
                'type': 'large_file',
                'severity': 'minor',
                'description': f'æ–‡ä»¶è¿‡å¤§: {len(non_empty_lines)} è¡Œä»£ç ',
                'code': ''
            })
        
        if max_indent > 6:
            issues.append({
                'file': relative_path,
                'line': 1,
                'type': 'deep_nesting',
                'severity': 'minor',
                'description': f'åµŒå¥—å±‚æ¬¡è¿‡æ·±: {max_indent} å±‚',
                'code': ''
            })
    
    return issues, file_stats

def analyze_code_duplication(project_root):
    """åˆ†æä»£ç é‡å¤"""
    issues = []
    line_hashes = defaultdict(list)
    
    for file_path in get_python_files(project_root):
        content = read_file_safe(file_path)
        if not content:
            continue
        
        lines = content.split('\n')
        relative_path = str(file_path.relative_to(Path(project_root)))
        
        for line_num, line in enumerate(lines, 1):
            clean_line = line.strip()
            if clean_line and not clean_line.startswith('#') and len(clean_line) > 20:
                line_hash = hash(clean_line)
                line_hashes[line_hash].append({
                    'file': relative_path,
                    'line': line_num,
                    'content': clean_line
                })
    
    # æŸ¥æ‰¾é‡å¤è¡Œ
    for line_hash, occurrences in line_hashes.items():
        if len(occurrences) > 1:
            for occurrence in occurrences[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ª
                issues.append({
                    'file': occurrence['file'],
                    'line': occurrence['line'],
                    'type': 'code_duplication',
                    'severity': 'minor',
                    'description': f'é‡å¤ä»£ç è¡Œï¼Œå…±{len(occurrences)}å¤„',
                    'code': occurrence['content'][:80] + '...' if len(occurrence['content']) > 80 else occurrence['content']
                })
    
    return issues

def analyze_error_handling(project_root):
    """åˆ†æé”™è¯¯å¤„ç†"""
    issues = []
    
    for file_path in get_python_files(project_root):
        content = read_file_safe(file_path)
        if not content:
            continue
        
        lines = content.split('\n')
        relative_path = str(file_path.relative_to(Path(project_root)))
        
        in_try_block = False
        try_line = 0
        
        for line_num, line in enumerate(lines, 1):
            stripped = line.strip()
            
            if stripped.startswith('try:'):
                in_try_block = True
                try_line = line_num
            elif stripped.startswith('except:') and in_try_block:
                issues.append({
                    'file': relative_path,
                    'line': line_num,
                    'type': 'bare_except',
                    'severity': 'major',
                    'description': 'ä½¿ç”¨äº†è£¸éœ²çš„exceptè¯­å¥',
                    'code': stripped
                })
                in_try_block = False
            elif stripped.startswith('except ') and in_try_block:
                in_try_block = False
            elif not stripped or not stripped.startswith(' '):
                in_try_block = False
    
    return issues

def generate_report(project_root):
    """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
    print("ğŸ” å¼€å§‹ä»£ç è´¨é‡åˆ†æ...")
    
    # æ‰§è¡Œå„ç§åˆ†æ
    print("ğŸ“‹ åˆ†æç¡¬ç¼–ç å¯¼å…¥...")
    hardcoded_issues = analyze_hardcoded_imports(project_root)
    
    print("ğŸ“Š åˆ†æä»£ç å¤æ‚åº¦...")
    complexity_issues, file_stats = analyze_code_complexity(project_root)
    
    print("ğŸ”„ åˆ†æä»£ç é‡å¤...")
    duplication_issues = analyze_code_duplication(project_root)
    
    print("âš ï¸  åˆ†æé”™è¯¯å¤„ç†...")
    error_handling_issues = analyze_error_handling(project_root)
    
    # åˆå¹¶æ‰€æœ‰é—®é¢˜
    all_issues = hardcoded_issues + complexity_issues + duplication_issues + error_handling_issues
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_files = len(get_python_files(project_root))
    total_lines = sum(stats['lines_of_code'] for stats in file_stats.values())
    
    # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
    severity_counts = Counter(issue['severity'] for issue in all_issues)
    type_counts = Counter(issue['type'] for issue in all_issues)
    
    # ç”ŸæˆæŠ¥å‘Š
    report_lines = [
        "# ğŸ” ä»£ç è´¨é‡åˆ†ææŠ¥å‘Š",
        f"\n**åˆ†ææ—¶é—´**: {__import__('time').strftime('%Y-%m-%d %H:%M:%S')}",
        f"**é¡¹ç›®æ–‡ä»¶æ•°**: {total_files}",
        f"**ä»£ç è¡Œæ•°**: {total_lines:,}",
        f"\n## ğŸ“Š é—®é¢˜ç»Ÿè®¡",
        f"\n- **æ€»é—®é¢˜æ•°**: {len(all_issues)}",
    ]
    
    # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
    for severity, count in severity_counts.items():
        emoji = {'critical': 'ğŸ”´', 'major': 'ğŸŸ¡', 'minor': 'ğŸŸ¢'}.get(severity, 'âšª')
        report_lines.append(f"- **{severity.title()}**: {count} {emoji}")
    
    # é—®é¢˜ç±»å‹ç»Ÿè®¡
    report_lines.append("\n## ğŸ“‹ é—®é¢˜ç±»å‹åˆ†å¸ƒ")
    for issue_type, count in type_counts.most_common():
        report_lines.append(f"- **{issue_type.replace('_', ' ').title()}**: {count}")
    
    # æ–‡ä»¶ç»Ÿè®¡
    report_lines.append("\n## ğŸ“ˆ ä»£ç ç»Ÿè®¡")
    total_functions = sum(stats['function_count'] for stats in file_stats.values())
    total_classes = sum(stats['class_count'] for stats in file_stats.values())
    avg_file_size = total_lines / total_files if total_files > 0 else 0
    
    report_lines.extend([
        f"- **å‡½æ•°æ€»æ•°**: {total_functions}",
        f"- **ç±»æ€»æ•°**: {total_classes}",
        f"- **å¹³å‡æ–‡ä»¶å¤§å°**: {avg_file_size:.1f} è¡Œ"
    ])
    
    # å…³é”®é—®é¢˜è¯¦æƒ…
    critical_issues = [issue for issue in all_issues if issue['severity'] in ['critical', 'major']]
    if critical_issues:
        report_lines.append("\n## ğŸ”¥ å…³é”®é—®é¢˜è¯¦æƒ…")
        
        for issue in critical_issues[:15]:  # åªæ˜¾ç¤ºå‰15ä¸ª
            report_lines.extend([
                f"\n### {issue['type'].replace('_', ' ').title()}",
                f"**æ–‡ä»¶**: `{issue['file']}:{issue['line']}`",
                f"**ä¸¥é‡ç¨‹åº¦**: {issue['severity'].upper()}",
                f"**æè¿°**: {issue['description']}"
            ])
            
            if issue['code']:
                report_lines.append(f"**ä»£ç **: `{issue['code']}`")
    
    # æ”¹è¿›å»ºè®®
    report_lines.extend([
        "\n## ğŸ’¡ æ”¹è¿›å»ºè®®",
        "\n### ç«‹å³è¡ŒåŠ¨é¡¹"
    ])
    
    if len([i for i in all_issues if i['type'] == 'hardcoded_import']) > 10:
        report_lines.append("1. **é‡æ„å¯¼å…¥ç³»ç»Ÿ**: æ¶ˆé™¤ç¡¬ç¼–ç å¯¼å…¥ï¼Œä½¿ç”¨ç›¸å¯¹å¯¼å…¥")
    
    if len([i for i in all_issues if i['type'] == 'bare_except']) > 0:
        report_lines.append("2. **æ”¹è¿›é”™è¯¯å¤„ç†**: ä½¿ç”¨å…·ä½“çš„å¼‚å¸¸ç±»å‹è€Œä¸æ˜¯è£¸éœ²çš„except")
    
    if len([i for i in all_issues if i['type'] == 'code_duplication']) > 20:
        report_lines.append("3. **æ¶ˆé™¤ä»£ç é‡å¤**: æå–å…¬å…±å‡½æ•°å’Œç±»")
    
    if avg_file_size > 300:
        report_lines.append("4. **æ‹†åˆ†å¤§æ–‡ä»¶**: å°†å¤§æ–‡ä»¶æ‹†åˆ†ä¸ºæ›´å°çš„æ¨¡å—")
    
    report_lines.extend([
        "\n### é•¿æœŸæ”¹è¿›",
        "1. **å»ºç«‹ä»£ç è´¨é‡é—¨ç¦**: åœ¨CI/CDä¸­é›†æˆä»£ç è´¨é‡æ£€æŸ¥",
        "2. **å®šæœŸé‡æ„**: å»ºç«‹å®šæœŸçš„æŠ€æœ¯å€ºåŠ¡æ¸…ç†è®¡åˆ’",
        "3. **å›¢é˜ŸåŸ¹è®­**: æå‡å›¢é˜Ÿçš„ä»£ç è´¨é‡æ„è¯†å’ŒæŠ€èƒ½",
        "4. **å·¥å…·åŒ–**: ä½¿ç”¨æ›´å¤šè‡ªåŠ¨åŒ–å·¥å…·æ¥ç»´æŠ¤ä»£ç è´¨é‡"
    ])
    
    # è®¡ç®—è´¨é‡è¯„åˆ†
    max_score = 100
    penalty = min(len(all_issues) * 2, max_score)  # æ¯ä¸ªé—®é¢˜æ‰£2åˆ†
    quality_score = max(0, max_score - penalty)
    
    report_lines.extend([
        f"\n## ğŸ¯ è´¨é‡è¯„åˆ†",
        f"\n**æ€»ä½“è¯„åˆ†**: {quality_score}/100"
    ])
    
    if quality_score < 50:
        report_lines.append("\nâš ï¸  **ä»£ç è´¨é‡è¾ƒå·®ï¼Œå»ºè®®ç«‹å³å¼€å§‹é‡æ„å·¥ä½œ**")
    elif quality_score < 70:
        report_lines.append("\nğŸ’¡ **ä»£ç è´¨é‡ä¸­ç­‰ï¼Œå»ºè®®åˆ¶å®šæ”¹è¿›è®¡åˆ’**")
    else:
        report_lines.append("\nâœ… **ä»£ç è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ**")
    
    return "\n".join(report_lines), {
        'total_files': total_files,
        'total_lines': total_lines,
        'total_issues': len(all_issues),
        'quality_score': quality_score,
        'severity_counts': dict(severity_counts),
        'type_counts': dict(type_counts)
    }

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    project_root = sys.argv[1] if len(sys.argv) > 1 else '.'
    output_file = None
    
    # ç®€å•çš„å‚æ•°è§£æ
    if '-o' in sys.argv:
        idx = sys.argv.index('-o')
        if idx + 1 < len(sys.argv):
            output_file = sys.argv[idx + 1]
    
    try:
        report_content, stats = generate_report(project_root)
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        else:
            print(report_content)
        
        # è¾“å‡ºç®€è¦ç»Ÿè®¡
        print(f"\nğŸ“Š åˆ†æå®Œæˆ:")
        print(f"   - æ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"   - ä»£ç è¡Œæ•°: {stats['total_lines']:,}")
        print(f"   - é—®é¢˜æ•°: {stats['total_issues']}")
        print(f"   - è´¨é‡è¯„åˆ†: {stats['quality_score']}/100")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()