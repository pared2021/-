#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»£ç è´¨é‡åˆ†æå·¥å…·
è‡ªåŠ¨åŒ–è¯†åˆ«æŠ€æœ¯å€ºåŠ¡ã€æ¶æ„é—®é¢˜å’Œä»£ç è´¨é‡æŒ‡æ ‡
"""

import os
import ast
import re
import json
import time
from typing import Dict, List, Tuple, Set, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
from collections import defaultdict, Counter
import subprocess
import sys

# ============================================================================
# æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================

@dataclass
class CodeIssue:
    """ä»£ç é—®é¢˜æ•°æ®ç±»"""
    file_path: str
    line_number: int
    issue_type: str
    severity: str  # 'critical', 'major', 'minor'
    description: str
    suggestion: str
    code_snippet: str

@dataclass
class DependencyInfo:
    """ä¾èµ–ä¿¡æ¯"""
    from_module: str
    to_module: str
    import_type: str  # 'absolute', 'relative', 'hardcoded'
    line_number: int
    is_circular: bool = False

@dataclass
class ComplexityMetrics:
    """å¤æ‚åº¦æŒ‡æ ‡"""
    cyclomatic_complexity: int
    cognitive_complexity: int
    lines_of_code: int
    function_count: int
    class_count: int
    nesting_depth: int

@dataclass
class QualityReport:
    """è´¨é‡æŠ¥å‘Š"""
    timestamp: str
    total_files: int
    total_lines: int
    issues: List[CodeIssue]
    dependencies: List[DependencyInfo]
    complexity_metrics: Dict[str, ComplexityMetrics]
    technical_debt_score: float
    maintainability_index: float
    test_coverage: float
    summary: Dict[str, Any]

# ============================================================================
# ä»£ç åˆ†æå™¨åŸºç±»
# ============================================================================

class BaseAnalyzer:
    """åˆ†æå™¨åŸºç±»"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.issues: List[CodeIssue] = []
        self.dependencies: List[DependencyInfo] = []
    
    def analyze(self) -> List[CodeIssue]:
        """æ‰§è¡Œåˆ†æ"""
        raise NotImplementedError
    
    def _get_python_files(self) -> List[Path]:
        """è·å–æ‰€æœ‰Pythonæ–‡ä»¶"""
        python_files = []
        for root, dirs, files in os.walk(self.project_root):
            # è·³è¿‡è™šæ‹Ÿç¯å¢ƒå’Œç¼“å­˜ç›®å½•
            dirs[:] = [d for d in dirs if d not in {'.git', '__pycache__', '.pytest_cache', 'venv', 'env', '.venv'}]
            
            for file in files:
                if file.endswith('.py'):
                    python_files.append(Path(root) / file)
        
        return python_files
    
    def _read_file_safe(self, file_path: Path) -> Optional[str]:
        """å®‰å…¨è¯»å–æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except (UnicodeDecodeError, FileNotFoundError):
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    return f.read()
            except Exception:
                return None
    
    def _create_issue(self, file_path: str, line_number: int, issue_type: str, 
                     severity: str, description: str, suggestion: str, 
                     code_snippet: str = "") -> CodeIssue:
        """åˆ›å»ºä»£ç é—®é¢˜"""
        return CodeIssue(
            file_path=file_path,
            line_number=line_number,
            issue_type=issue_type,
            severity=severity,
            description=description,
            suggestion=suggestion,
            code_snippet=code_snippet
        )

# ============================================================================
# ç¡¬ç¼–ç å¯¼å…¥åˆ†æå™¨
# ============================================================================

class HardcodedImportAnalyzer(BaseAnalyzer):
    """ç¡¬ç¼–ç å¯¼å…¥åˆ†æå™¨"""
    
    def __init__(self, project_root: str):
        super().__init__(project_root)
        self.hardcoded_patterns = [
            r'from\s+src\.',
            r'import\s+src\.',
            r'sys\.path\.append',
            r'os\.path\.dirname.*__file__'
        ]
    
    def analyze(self) -> List[CodeIssue]:
        """åˆ†æç¡¬ç¼–ç å¯¼å…¥"""
        issues = []
        
        for file_path in self._get_python_files():
            content = self._read_file_safe(file_path)
            if not content:
                continue
            
            lines = content.split('\n')
            for line_num, line in enumerate(lines, 1):
                for pattern in self.hardcoded_patterns:
                    if re.search(pattern, line):
                        issues.append(self._create_issue(
                            file_path=str(file_path.relative_to(self.project_root)),
                            line_number=line_num,
                            issue_type="hardcoded_import",
                            severity="major",
                            description=f"ç¡¬ç¼–ç å¯¼å…¥: {line.strip()}",
                            suggestion="ä½¿ç”¨ç›¸å¯¹å¯¼å…¥æˆ–é…ç½®åŒ–çš„æ¨¡å—è·¯å¾„",
                            code_snippet=line.strip()
                        ))
        
        return issues

# ============================================================================
# ä¾èµ–åˆ†æå™¨
# ============================================================================

class DependencyAnalyzer(BaseAnalyzer):
    """ä¾èµ–å…³ç³»åˆ†æå™¨"""
    
    def analyze(self) -> Tuple[List[DependencyInfo], List[CodeIssue]]:
        """åˆ†æä¾èµ–å…³ç³»"""
        dependencies = []
        issues = []
        module_imports = defaultdict(set)
        
        for file_path in self._get_python_files():
            content = self._read_file_safe(file_path)
            if not content:
                continue
            
            try:
                tree = ast.parse(content)
                module_name = self._get_module_name(file_path)
                
                for node in ast.walk(tree):
                    if isinstance(node, (ast.Import, ast.ImportFrom)):
                        dep_info = self._extract_dependency_info(node, file_path, module_name)
                        if dep_info:
                            dependencies.append(dep_info)
                            module_imports[module_name].add(dep_info.to_module)
                            
            except SyntaxError as e:
                issues.append(self._create_issue(
                    file_path=str(file_path.relative_to(self.project_root)),
                    line_number=e.lineno or 0,
                    issue_type="syntax_error",
                    severity="critical",
                    description=f"è¯­æ³•é”™è¯¯: {e.msg}",
                    suggestion="ä¿®å¤è¯­æ³•é”™è¯¯"
                ))
        
        # æ£€æµ‹å¾ªç¯ä¾èµ–
        circular_deps = self._detect_circular_dependencies(module_imports)
        for dep in dependencies:
            if (dep.from_module, dep.to_module) in circular_deps:
                dep.is_circular = True
                issues.append(self._create_issue(
                    file_path=dep.from_module,
                    line_number=dep.line_number,
                    issue_type="circular_dependency",
                    severity="major",
                    description=f"å¾ªç¯ä¾èµ–: {dep.from_module} -> {dep.to_module}",
                    suggestion="é‡æ„ä»£ç ä»¥æ¶ˆé™¤å¾ªç¯ä¾èµ–"
                ))
        
        return dependencies, issues
    
    def _get_module_name(self, file_path: Path) -> str:
        """è·å–æ¨¡å—å"""
        relative_path = file_path.relative_to(self.project_root)
        return str(relative_path).replace(os.sep, '.').replace('.py', '')
    
    def _extract_dependency_info(self, node: ast.AST, file_path: Path, module_name: str) -> Optional[DependencyInfo]:
        """æå–ä¾èµ–ä¿¡æ¯"""
        if isinstance(node, ast.Import):
            for alias in node.names:
                return DependencyInfo(
                    from_module=module_name,
                    to_module=alias.name,
                    import_type="absolute",
                    line_number=node.lineno
                )
        elif isinstance(node, ast.ImportFrom):
            if node.module:
                import_type = "relative" if node.level > 0 else "absolute"
                if node.module.startswith('src.'):
                    import_type = "hardcoded"
                
                return DependencyInfo(
                    from_module=module_name,
                    to_module=node.module,
                    import_type=import_type,
                    line_number=node.lineno
                )
        
        return None
    
    def _detect_circular_dependencies(self, module_imports: Dict[str, Set[str]]) -> Set[Tuple[str, str]]:
        """æ£€æµ‹å¾ªç¯ä¾èµ–"""
        circular = set()
        
        def has_path(start: str, end: str, visited: Set[str]) -> bool:
            if start == end:
                return True
            if start in visited:
                return False
            
            visited.add(start)
            for neighbor in module_imports.get(start, set()):
                if has_path(neighbor, end, visited.copy()):
                    return True
            return False
        
        for module in module_imports:
            for imported in module_imports[module]:
                if has_path(imported, module, set()):
                    circular.add((module, imported))
        
        return circular

# ============================================================================
# å¤æ‚åº¦åˆ†æå™¨
# ============================================================================

class ComplexityAnalyzer(BaseAnalyzer):
    """å¤æ‚åº¦åˆ†æå™¨"""
    
    def analyze(self) -> Tuple[Dict[str, ComplexityMetrics], List[CodeIssue]]:
        """åˆ†æä»£ç å¤æ‚åº¦"""
        metrics = {}
        issues = []
        
        for file_path in self._get_python_files():
            content = self._read_file_safe(file_path)
            if not content:
                continue
            
            try:
                tree = ast.parse(content)
                file_metrics = self._calculate_file_metrics(tree, content)
                relative_path = str(file_path.relative_to(self.project_root))
                metrics[relative_path] = file_metrics
                
                # æ£€æŸ¥å¤æ‚åº¦é˜ˆå€¼
                if file_metrics.cyclomatic_complexity > 10:
                    issues.append(self._create_issue(
                        file_path=relative_path,
                        line_number=1,
                        issue_type="high_complexity",
                        severity="major",
                        description=f"åœˆå¤æ‚åº¦è¿‡é«˜: {file_metrics.cyclomatic_complexity}",
                        suggestion="è€ƒè™‘æ‹†åˆ†å‡½æ•°æˆ–ç±»ä»¥é™ä½å¤æ‚åº¦"
                    ))
                
                if file_metrics.nesting_depth > 4:
                    issues.append(self._create_issue(
                        file_path=relative_path,
                        line_number=1,
                        issue_type="deep_nesting",
                        severity="minor",
                        description=f"åµŒå¥—å±‚æ¬¡è¿‡æ·±: {file_metrics.nesting_depth}",
                        suggestion="ä½¿ç”¨æ—©æœŸè¿”å›æˆ–æå–å‡½æ•°æ¥å‡å°‘åµŒå¥—"
                    ))
                
            except SyntaxError:
                continue
        
        return metrics, issues
    
    def _calculate_file_metrics(self, tree: ast.AST, content: str) -> ComplexityMetrics:
        """è®¡ç®—æ–‡ä»¶æŒ‡æ ‡"""
        lines = content.split('\n')
        non_empty_lines = [line for line in lines if line.strip() and not line.strip().startswith('#')]
        
        function_count = 0
        class_count = 0
        max_nesting = 0
        total_complexity = 1  # åŸºç¡€å¤æ‚åº¦
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                function_count += 1
                total_complexity += self._calculate_cyclomatic_complexity(node)
            elif isinstance(node, ast.ClassDef):
                class_count += 1
            
            # è®¡ç®—åµŒå¥—æ·±åº¦
            nesting = self._calculate_nesting_depth(node)
            max_nesting = max(max_nesting, nesting)
        
        return ComplexityMetrics(
            cyclomatic_complexity=total_complexity,
            cognitive_complexity=total_complexity,  # ç®€åŒ–å®ç°
            lines_of_code=len(non_empty_lines),
            function_count=function_count,
            class_count=class_count,
            nesting_depth=max_nesting
        )
    
    def _calculate_cyclomatic_complexity(self, node: ast.FunctionDef) -> int:
        """è®¡ç®—åœˆå¤æ‚åº¦"""
        complexity = 1
        
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
        
        return complexity
    
    def _calculate_nesting_depth(self, node: ast.AST) -> int:
        """è®¡ç®—åµŒå¥—æ·±åº¦"""
        if not hasattr(node, 'body'):
            return 0
        
        max_depth = 0
        for child in getattr(node, 'body', []):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor, ast.With, ast.Try)):
                depth = 1 + self._calculate_nesting_depth(child)
                max_depth = max(max_depth, depth)
        
        return max_depth

# ============================================================================
# ä»£ç é‡å¤åˆ†æå™¨
# ============================================================================

class DuplicationAnalyzer(BaseAnalyzer):
    """ä»£ç é‡å¤åˆ†æå™¨"""
    
    def __init__(self, project_root: str, min_lines: int = 5):
        super().__init__(project_root)
        self.min_lines = min_lines
    
    def analyze(self) -> List[CodeIssue]:
        """åˆ†æä»£ç é‡å¤"""
        issues = []
        code_blocks = self._extract_code_blocks()
        duplicates = self._find_duplicates(code_blocks)
        
        for duplicate_group in duplicates:
            if len(duplicate_group) > 1:
                for block in duplicate_group[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªï¼Œå…¶ä»–éƒ½æ˜¯é‡å¤
                    issues.append(self._create_issue(
                        file_path=block['file'],
                        line_number=block['start_line'],
                        issue_type="code_duplication",
                        severity="minor",
                        description=f"ä»£ç é‡å¤ï¼Œå…±{len(duplicate_group)}å¤„",
                        suggestion="æå–å…¬å…±å‡½æ•°æˆ–ç±»æ¥æ¶ˆé™¤é‡å¤",
                        code_snippet=block['content'][:100] + "..."
                    ))
        
        return issues
    
    def _extract_code_blocks(self) -> List[Dict[str, Any]]:
        """æå–ä»£ç å—"""
        blocks = []
        
        for file_path in self._get_python_files():
            content = self._read_file_safe(file_path)
            if not content:
                continue
            
            lines = content.split('\n')
            for i in range(len(lines) - self.min_lines + 1):
                block_lines = lines[i:i + self.min_lines]
                # è¿‡æ»¤ç©ºè¡Œå’Œæ³¨é‡Š
                meaningful_lines = [line.strip() for line in block_lines 
                                  if line.strip() and not line.strip().startswith('#')]
                
                if len(meaningful_lines) >= self.min_lines:
                    blocks.append({
                        'file': str(file_path.relative_to(self.project_root)),
                        'start_line': i + 1,
                        'content': '\n'.join(meaningful_lines),
                        'hash': hash('\n'.join(meaningful_lines))
                    })
        
        return blocks
    
    def _find_duplicates(self, blocks: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
        """æŸ¥æ‰¾é‡å¤ä»£ç å—"""
        hash_groups = defaultdict(list)
        
        for block in blocks:
            hash_groups[block['hash']].append(block)
        
        return [group for group in hash_groups.values() if len(group) > 1]

# ============================================================================
# æµ‹è¯•è¦†ç›–ç‡åˆ†æå™¨
# ============================================================================

class TestCoverageAnalyzer:
    """æµ‹è¯•è¦†ç›–ç‡åˆ†æå™¨"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
    
    def analyze(self) -> float:
        """åˆ†ææµ‹è¯•è¦†ç›–ç‡"""
        try:
            # å°è¯•è¿è¡Œcoverage
            result = subprocess.run(
                [sys.executable, '-m', 'coverage', 'run', '-m', 'pytest'],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300
            )
            
            if result.returncode == 0:
                # è·å–è¦†ç›–ç‡æŠ¥å‘Š
                coverage_result = subprocess.run(
                    [sys.executable, '-m', 'coverage', 'report', '--format=json'],
                    cwd=self.project_root,
                    capture_output=True,
                    text=True
                )
                
                if coverage_result.returncode == 0:
                    coverage_data = json.loads(coverage_result.stdout)
                    return coverage_data.get('totals', {}).get('percent_covered', 0.0)
            
        except (subprocess.TimeoutExpired, json.JSONDecodeError, FileNotFoundError):
            pass
        
        # å¦‚æœæ— æ³•è·å–å®é™…è¦†ç›–ç‡ï¼Œä¼°ç®—æµ‹è¯•æ–‡ä»¶æ¯”ä¾‹
        return self._estimate_test_coverage()
    
    def _estimate_test_coverage(self) -> float:
        """ä¼°ç®—æµ‹è¯•è¦†ç›–ç‡"""
        total_files = 0
        test_files = 0
        
        for root, dirs, files in os.walk(self.project_root):
            dirs[:] = [d for d in dirs if d not in {'.git', '__pycache__', '.pytest_cache'}]
            
            for file in files:
                if file.endswith('.py'):
                    total_files += 1
                    if 'test' in file.lower() or 'test' in root.lower():
                        test_files += 1
        
        return (test_files / total_files * 100) if total_files > 0 else 0.0

# ============================================================================
# ä¸»åˆ†æå™¨
# ============================================================================

class CodeQualityAnalyzer:
    """ä»£ç è´¨é‡åˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.analyzers = {
            'hardcoded_imports': HardcodedImportAnalyzer(project_root),
            'dependencies': DependencyAnalyzer(project_root),
            'complexity': ComplexityAnalyzer(project_root),
            'duplication': DuplicationAnalyzer(project_root),
            'test_coverage': TestCoverageAnalyzer(project_root)
        }
    
    def analyze(self) -> QualityReport:
        """æ‰§è¡Œå®Œæ•´çš„ä»£ç è´¨é‡åˆ†æ"""
        print("ğŸ” å¼€å§‹ä»£ç è´¨é‡åˆ†æ...")
        start_time = time.time()
        
        all_issues = []
        all_dependencies = []
        complexity_metrics = {}
        
        # 1. ç¡¬ç¼–ç å¯¼å…¥åˆ†æ
        print("ğŸ“‹ åˆ†æç¡¬ç¼–ç å¯¼å…¥...")
        hardcoded_issues = self.analyzers['hardcoded_imports'].analyze()
        all_issues.extend(hardcoded_issues)
        
        # 2. ä¾èµ–å…³ç³»åˆ†æ
        print("ğŸ”— åˆ†æä¾èµ–å…³ç³»...")
        dependencies, dep_issues = self.analyzers['dependencies'].analyze()
        all_dependencies.extend(dependencies)
        all_issues.extend(dep_issues)
        
        # 3. å¤æ‚åº¦åˆ†æ
        print("ğŸ“Š åˆ†æä»£ç å¤æ‚åº¦...")
        complexity_metrics, complexity_issues = self.analyzers['complexity'].analyze()
        all_issues.extend(complexity_issues)
        
        # 4. ä»£ç é‡å¤åˆ†æ
        print("ğŸ”„ åˆ†æä»£ç é‡å¤...")
        duplication_issues = self.analyzers['duplication'].analyze()
        all_issues.extend(duplication_issues)
        
        # 5. æµ‹è¯•è¦†ç›–ç‡åˆ†æ
        print("ğŸ§ª åˆ†ææµ‹è¯•è¦†ç›–ç‡...")
        test_coverage = self.analyzers['test_coverage'].analyze()
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        technical_debt_score = self._calculate_technical_debt_score(all_issues)
        maintainability_index = self._calculate_maintainability_index(
            complexity_metrics, len(all_issues), test_coverage
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_files = len(list(self.project_root.rglob('*.py')))
        total_lines = sum(metrics.lines_of_code for metrics in complexity_metrics.values())
        
        analysis_time = time.time() - start_time
        print(f"âœ… åˆ†æå®Œæˆï¼Œè€—æ—¶ {analysis_time:.2f} ç§’")
        
        return QualityReport(
            timestamp=time.strftime('%Y-%m-%d %H:%M:%S'),
            total_files=total_files,
            total_lines=total_lines,
            issues=all_issues,
            dependencies=all_dependencies,
            complexity_metrics=complexity_metrics,
            technical_debt_score=technical_debt_score,
            maintainability_index=maintainability_index,
            test_coverage=test_coverage,
            summary=self._generate_summary(all_issues, dependencies, complexity_metrics)
        )
    
    def _calculate_technical_debt_score(self, issues: List[CodeIssue]) -> float:
        """è®¡ç®—æŠ€æœ¯å€ºåŠ¡è¯„åˆ†"""
        severity_weights = {'critical': 10, 'major': 5, 'minor': 1}
        total_weight = sum(severity_weights.get(issue.severity, 1) for issue in issues)
        
        # å½’ä¸€åŒ–åˆ°0-100åˆ†ï¼Œåˆ†æ•°è¶Šä½è¡¨ç¤ºå€ºåŠ¡è¶Šå¤š
        max_possible_score = 100
        debt_penalty = min(total_weight, max_possible_score)
        
        return max(0, max_possible_score - debt_penalty)
    
    def _calculate_maintainability_index(self, complexity_metrics: Dict[str, ComplexityMetrics], 
                                       issue_count: int, test_coverage: float) -> float:
        """è®¡ç®—å¯ç»´æŠ¤æ€§æŒ‡æ•°"""
        if not complexity_metrics:
            return 0.0
        
        # å¹³å‡å¤æ‚åº¦
        avg_complexity = sum(m.cyclomatic_complexity for m in complexity_metrics.values()) / len(complexity_metrics)
        
        # å¯ç»´æŠ¤æ€§æŒ‡æ•°å…¬å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # MI = 171 - 5.2 * ln(Halstead Volume) - 0.23 * (Cyclomatic Complexity) - 16.2 * ln(Lines of Code)
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–å…¬å¼
        complexity_factor = max(0, 100 - avg_complexity * 5)
        issue_factor = max(0, 100 - issue_count)
        coverage_factor = test_coverage
        
        return (complexity_factor + issue_factor + coverage_factor) / 3
    
    def _generate_summary(self, issues: List[CodeIssue], dependencies: List[DependencyInfo], 
                         complexity_metrics: Dict[str, ComplexityMetrics]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        issue_counts = Counter(issue.issue_type for issue in issues)
        severity_counts = Counter(issue.severity for issue in issues)
        
        circular_deps = sum(1 for dep in dependencies if dep.is_circular)
        hardcoded_imports = sum(1 for dep in dependencies if dep.import_type == 'hardcoded')
        
        return {
            'total_issues': len(issues),
            'issue_types': dict(issue_counts),
            'severity_distribution': dict(severity_counts),
            'circular_dependencies': circular_deps,
            'hardcoded_imports': hardcoded_imports,
            'average_complexity': sum(m.cyclomatic_complexity for m in complexity_metrics.values()) / len(complexity_metrics) if complexity_metrics else 0,
            'total_functions': sum(m.function_count for m in complexity_metrics.values()),
            'total_classes': sum(m.class_count for m in complexity_metrics.values())
        }
    
    def generate_report(self, report: QualityReport, output_file: str = None) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report_content = self._format_report(report)
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        return report_content
    
    def _format_report(self, report: QualityReport) -> str:
        """æ ¼å¼åŒ–æŠ¥å‘Š"""
        lines = [
            "# ğŸ” ä»£ç è´¨é‡åˆ†ææŠ¥å‘Š",
            f"\n**åˆ†ææ—¶é—´**: {report.timestamp}",
            f"**é¡¹ç›®æ–‡ä»¶æ•°**: {report.total_files}",
            f"**ä»£ç è¡Œæ•°**: {report.total_lines:,}",
            f"\n## ğŸ“Š è´¨é‡æŒ‡æ ‡",
            f"\n- **æŠ€æœ¯å€ºåŠ¡è¯„åˆ†**: {report.technical_debt_score:.1f}/100",
            f"- **å¯ç»´æŠ¤æ€§æŒ‡æ•°**: {report.maintainability_index:.1f}/100",
            f"- **æµ‹è¯•è¦†ç›–ç‡**: {report.test_coverage:.1f}%",
            f"\n## ğŸš¨ é—®é¢˜ç»Ÿè®¡",
            f"\n- **æ€»é—®é¢˜æ•°**: {len(report.issues)}",
        ]
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        severity_counts = Counter(issue.severity for issue in report.issues)
        for severity, count in severity_counts.items():
            emoji = {'critical': 'ğŸ”´', 'major': 'ğŸŸ¡', 'minor': 'ğŸŸ¢'}.get(severity, 'âšª')
            lines.append(f"- **{severity.title()}**: {count} {emoji}")
        
        # é—®é¢˜ç±»å‹ç»Ÿè®¡
        lines.append("\n## ğŸ“‹ é—®é¢˜ç±»å‹åˆ†å¸ƒ")
        issue_types = Counter(issue.issue_type for issue in report.issues)
        for issue_type, count in issue_types.most_common():
            lines.append(f"- **{issue_type.replace('_', ' ').title()}**: {count}")
        
        # å¤æ‚åº¦ç»Ÿè®¡
        if report.complexity_metrics:
            lines.append("\n## ğŸ“ˆ å¤æ‚åº¦åˆ†æ")
            avg_complexity = sum(m.cyclomatic_complexity for m in report.complexity_metrics.values()) / len(report.complexity_metrics)
            total_functions = sum(m.function_count for m in report.complexity_metrics.values())
            total_classes = sum(m.class_count for m in report.complexity_metrics.values())
            
            lines.extend([
                f"- **å¹³å‡åœˆå¤æ‚åº¦**: {avg_complexity:.1f}",
                f"- **å‡½æ•°æ€»æ•°**: {total_functions}",
                f"- **ç±»æ€»æ•°**: {total_classes}"
            ])
        
        # ä¾èµ–å…³ç³»ç»Ÿè®¡
        lines.append("\n## ğŸ”— ä¾èµ–å…³ç³»åˆ†æ")
        circular_deps = sum(1 for dep in report.dependencies if dep.is_circular)
        hardcoded_imports = sum(1 for dep in report.dependencies if dep.import_type == 'hardcoded')
        
        lines.extend([
            f"- **æ€»ä¾èµ–æ•°**: {len(report.dependencies)}",
            f"- **å¾ªç¯ä¾èµ–**: {circular_deps}",
            f"- **ç¡¬ç¼–ç å¯¼å…¥**: {hardcoded_imports}"
        ])
        
        # è¯¦ç»†é—®é¢˜åˆ—è¡¨ï¼ˆåªæ˜¾ç¤ºå‰20ä¸ªæœ€ä¸¥é‡çš„é—®é¢˜ï¼‰
        critical_issues = [issue for issue in report.issues if issue.severity == 'critical']
        major_issues = [issue for issue in report.issues if issue.severity == 'major']
        
        if critical_issues or major_issues:
            lines.append("\n## ğŸ”¥ å…³é”®é—®é¢˜è¯¦æƒ…")
            
            for issue in (critical_issues + major_issues)[:20]:
                lines.extend([
                    f"\n### {issue.issue_type.replace('_', ' ').title()}",
                    f"**æ–‡ä»¶**: `{issue.file_path}:{issue.line_number}`",
                    f"**ä¸¥é‡ç¨‹åº¦**: {issue.severity.upper()}",
                    f"**æè¿°**: {issue.description}",
                    f"**å»ºè®®**: {issue.suggestion}"
                ])
                
                if issue.code_snippet:
                    lines.append(f"**ä»£ç ç‰‡æ®µ**:\n```python\n{issue.code_snippet}\n```")
        
        # æ”¹è¿›å»ºè®®
        lines.extend([
            "\n## ğŸ’¡ æ”¹è¿›å»ºè®®",
            "\n### ç«‹å³è¡ŒåŠ¨é¡¹",
        ])
        
        if critical_issues:
            lines.append("1. **ä¿®å¤å…³é”®é—®é¢˜**: ä¼˜å…ˆå¤„ç†æ‰€æœ‰criticalçº§åˆ«çš„é—®é¢˜")
        
        if hardcoded_imports > 10:
            lines.append("2. **é‡æ„å¯¼å…¥ç³»ç»Ÿ**: æ¶ˆé™¤ç¡¬ç¼–ç å¯¼å…¥ï¼Œä½¿ç”¨ç›¸å¯¹å¯¼å…¥")
        
        if circular_deps > 0:
            lines.append("3. **è§£å†³å¾ªç¯ä¾èµ–**: é‡æ„ä»£ç æ¶æ„ä»¥æ¶ˆé™¤å¾ªç¯ä¾èµ–")
        
        if report.test_coverage < 50:
            lines.append("4. **æå‡æµ‹è¯•è¦†ç›–ç‡**: ç¼–å†™æ›´å¤šå•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•")
        
        lines.extend([
            "\n### é•¿æœŸæ”¹è¿›",
            "1. **å»ºç«‹ä»£ç è´¨é‡é—¨ç¦**: åœ¨CI/CDä¸­é›†æˆä»£ç è´¨é‡æ£€æŸ¥",
            "2. **å®šæœŸé‡æ„**: å»ºç«‹å®šæœŸçš„æŠ€æœ¯å€ºåŠ¡æ¸…ç†è®¡åˆ’",
            "3. **å›¢é˜ŸåŸ¹è®­**: æå‡å›¢é˜Ÿçš„ä»£ç è´¨é‡æ„è¯†å’ŒæŠ€èƒ½",
            "4. **å·¥å…·åŒ–**: ä½¿ç”¨æ›´å¤šè‡ªåŠ¨åŒ–å·¥å…·æ¥ç»´æŠ¤ä»£ç è´¨é‡"
        ])
        
        return "\n".join(lines)

# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä»£ç è´¨é‡åˆ†æå·¥å…·')
    parser.add_argument('project_root', nargs='?', default='.', help='é¡¹ç›®æ ¹ç›®å½•')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--json', action='store_true', help='è¾“å‡ºJSONæ ¼å¼æŠ¥å‘Š')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    try:
        analyzer = CodeQualityAnalyzer(args.project_root)
        report = analyzer.analyze()
        
        if args.json:
            # è¾“å‡ºJSONæ ¼å¼
            json_data = {
                'timestamp': report.timestamp,
                'metrics': {
                    'technical_debt_score': report.technical_debt_score,
                    'maintainability_index': report.maintainability_index,
                    'test_coverage': report.test_coverage,
                    'total_files': report.total_files,
                    'total_lines': report.total_lines
                },
                'issues': [asdict(issue) for issue in report.issues],
                'summary': report.summary
            }
            
            output_content = json.dumps(json_data, ensure_ascii=False, indent=2)
        else:
            # è¾“å‡ºMarkdownæ ¼å¼
            output_content = analyzer.generate_report(report)
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(output_content)
            print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        else:
            print(output_content)
        
        # è¾“å‡ºç®€è¦ç»Ÿè®¡
        print(f"\nğŸ“Š åˆ†æå®Œæˆ:")
        print(f"   - æ–‡ä»¶æ•°: {report.total_files}")
        print(f"   - ä»£ç è¡Œæ•°: {report.total_lines:,}")
        print(f"   - é—®é¢˜æ•°: {len(report.issues)}")
        print(f"   - æŠ€æœ¯å€ºåŠ¡è¯„åˆ†: {report.technical_debt_score:.1f}/100")
        print(f"   - å¯ç»´æŠ¤æ€§æŒ‡æ•°: {report.maintainability_index:.1f}/100")
        print(f"   - æµ‹è¯•è¦†ç›–ç‡: {report.test_coverage:.1f}%")
        
        # æ ¹æ®è¯„åˆ†ç»™å‡ºå»ºè®®
        if report.technical_debt_score < 50:
            print("\nâš ï¸  æŠ€æœ¯å€ºåŠ¡è¾ƒé«˜ï¼Œå»ºè®®ç«‹å³å¼€å§‹é‡æ„å·¥ä½œ")
        elif report.technical_debt_score < 70:
            print("\nğŸ’¡ æŠ€æœ¯å€ºåŠ¡ä¸­ç­‰ï¼Œå»ºè®®åˆ¶å®šæ”¹è¿›è®¡åˆ’")
        else:
            print("\nâœ… ä»£ç è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()