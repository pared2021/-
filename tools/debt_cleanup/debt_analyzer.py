#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ€æœ¯å€ºåŠ¡åˆ†æå™¨

è‡ªåŠ¨åŒ–åˆ†æé¡¹ç›®ä¸­çš„æŠ€æœ¯å€ºåŠ¡ï¼ŒåŒ…æ‹¬ï¼š
- ç¡¬ç¼–ç å¯¼å…¥æ£€æµ‹
- ä»£ç é‡å¤åˆ†æ
- æ¶æ„åæ¨¡å¼è¯†åˆ«
- é”™è¯¯å¤„ç†ä¸€è‡´æ€§æ£€æŸ¥
"""

import os
import re
import ast
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any
from dataclasses import dataclass
from collections import defaultdict
import json


@dataclass
class DebtIssue:
    """æŠ€æœ¯å€ºåŠ¡é—®é¢˜"""
    type: str  # é—®é¢˜ç±»å‹
    severity: str  # ä¸¥é‡ç¨‹åº¦: critical, major, minor
    file_path: str  # æ–‡ä»¶è·¯å¾„
    line_number: int  # è¡Œå·
    description: str  # é—®é¢˜æè¿°
    suggestion: str  # ä¿®å¤å»ºè®®
    estimated_hours: float  # é¢„ä¼°ä¿®å¤æ—¶é—´


@dataclass
class DebtReport:
    """å€ºåŠ¡åˆ†ææŠ¥å‘Š"""
    total_issues: int
    critical_issues: int
    major_issues: int
    minor_issues: int
    estimated_total_hours: float
    issues_by_type: Dict[str, int]
    issues_by_file: Dict[str, int]
    detailed_issues: List[DebtIssue]


class DebtAnalyzer:
    """æŠ€æœ¯å€ºåŠ¡åˆ†æå™¨"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.src_path = self.project_root / "src"
        
        # é—®é¢˜ç±»å‹æƒé‡
        self.severity_weights = {
            "critical": 3.0,
            "major": 2.0, 
            "minor": 1.0
        }
        
        # ç¡¬ç¼–ç å¯¼å…¥æ¨¡å¼
        self.hardcoded_import_patterns = [
            r"from\s+src\.[\w.]+\s+import",
            r"import\s+src\.[\w.]+"
        ]
        
        # åæ¨¡å¼æ£€æµ‹è§„åˆ™
        self.antipatterns = {
            "service_locator": r"container\.(get|resolve)\(",
            "god_class": None,  # éœ€è¦ASTåˆ†æ
            "long_method": None,  # éœ€è¦ASTåˆ†æ
            "duplicate_code": None  # éœ€è¦ç‰¹æ®Šç®—æ³•
        }
    
    def analyze_project(self) -> DebtReport:
        """åˆ†ææ•´ä¸ªé¡¹ç›®çš„æŠ€æœ¯å€ºåŠ¡"""
        issues = []
        
        # åˆ†ææ‰€æœ‰Pythonæ–‡ä»¶
        for py_file in self._get_python_files():
            issues.extend(self._analyze_file(py_file))
        
        # ç”ŸæˆæŠ¥å‘Š
        return self._generate_report(issues)
    
    def _get_python_files(self) -> List[Path]:
        """è·å–æ‰€æœ‰Pythonæ–‡ä»¶"""
        python_files = []
        
        for root, dirs, files in os.walk(self.src_path):
            # è·³è¿‡__pycache__ç›®å½•
            dirs[:] = [d for d in dirs if d != "__pycache__"]
            
            for file in files:
                if file.endswith(".py"):
                    python_files.append(Path(root) / file)
        
        return python_files
    
    def _analyze_file(self, file_path: Path) -> List[DebtIssue]:
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„æŠ€æœ¯å€ºåŠ¡"""
        issues = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.splitlines()
            
            # æ£€æµ‹ç¡¬ç¼–ç å¯¼å…¥
            issues.extend(self._detect_hardcoded_imports(file_path, lines))
            
            # æ£€æµ‹åæ¨¡å¼
            issues.extend(self._detect_antipatterns(file_path, lines))
            
            # ASTåˆ†æ
            try:
                tree = ast.parse(content)
                issues.extend(self._analyze_ast(file_path, tree))
            except SyntaxError:
                # è¯­æ³•é”™è¯¯æ–‡ä»¶è·³è¿‡ASTåˆ†æ
                pass
                
        except Exception as e:
            # è®°å½•æ–‡ä»¶è¯»å–é”™è¯¯
            issues.append(DebtIssue(
                type="file_error",
                severity="minor",
                file_path=str(file_path),
                line_number=1,
                description=f"æ–‡ä»¶è¯»å–é”™è¯¯: {e}",
                suggestion="æ£€æŸ¥æ–‡ä»¶ç¼–ç å’Œæƒé™",
                estimated_hours=0.1
            ))
        
        return issues
    
    def _detect_hardcoded_imports(self, file_path: Path, lines: List[str]) -> List[DebtIssue]:
        """æ£€æµ‹ç¡¬ç¼–ç å¯¼å…¥"""
        issues = []
        
        for line_num, line in enumerate(lines, 1):
            for pattern in self.hardcoded_import_patterns:
                if re.search(pattern, line.strip()):
                    issues.append(DebtIssue(
                        type="hardcoded_import",
                        severity="major",
                        file_path=str(file_path),
                        line_number=line_num,
                        description=f"ç¡¬ç¼–ç å¯¼å…¥: {line.strip()}",
                        suggestion="ä½¿ç”¨ç›¸å¯¹å¯¼å…¥æˆ–ä¾èµ–æ³¨å…¥",
                        estimated_hours=0.5
                    ))
        
        return issues
    
    def _detect_antipatterns(self, file_path: Path, lines: List[str]) -> List[DebtIssue]:
        """æ£€æµ‹æ¶æ„åæ¨¡å¼"""
        issues = []
        
        for line_num, line in enumerate(lines, 1):
            # æ£€æµ‹æœåŠ¡å®šä½å™¨æ¨¡å¼
            if re.search(self.antipatterns["service_locator"], line):
                issues.append(DebtIssue(
                    type="service_locator_antipattern",
                    severity="critical",
                    file_path=str(file_path),
                    line_number=line_num,
                    description="ä½¿ç”¨æœåŠ¡å®šä½å™¨åæ¨¡å¼",
                    suggestion="æ”¹ç”¨ä¾èµ–æ³¨å…¥",
                    estimated_hours=2.0
                ))
        
        return issues
    
    def _analyze_ast(self, file_path: Path, tree: ast.AST) -> List[DebtIssue]:
        """ASTåˆ†ææ£€æµ‹å¤æ‚é—®é¢˜"""
        issues = []
        
        class DebtVisitor(ast.NodeVisitor):
            def __init__(self):
                self.issues = []
            
            def visit_FunctionDef(self, node):
                # æ£€æµ‹é•¿æ–¹æ³•
                if self._count_lines(node) > 50:
                    self.issues.append(DebtIssue(
                        type="long_method",
                        severity="major",
                        file_path=str(file_path),
                        line_number=node.lineno,
                        description=f"æ–¹æ³•è¿‡é•¿: {node.name} ({self._count_lines(node)} è¡Œ)",
                        suggestion="æ‹†åˆ†ä¸ºå¤šä¸ªå°æ–¹æ³•",
                        estimated_hours=1.5
                    ))
                
                self.generic_visit(node)
            
            def visit_ClassDef(self, node):
                # æ£€æµ‹ä¸Šå¸ç±»
                method_count = len([n for n in node.body if isinstance(n, ast.FunctionDef)])
                if method_count > 20:
                    self.issues.append(DebtIssue(
                        type="god_class",
                        severity="critical",
                        file_path=str(file_path),
                        line_number=node.lineno,
                        description=f"ä¸Šå¸ç±»: {node.name} ({method_count} ä¸ªæ–¹æ³•)",
                        suggestion="æ‹†åˆ†ä¸ºå¤šä¸ªèŒè´£å•ä¸€çš„ç±»",
                        estimated_hours=4.0
                    ))
                
                self.generic_visit(node)
            
            def _count_lines(self, node):
                """è®¡ç®—èŠ‚ç‚¹çš„è¡Œæ•°"""
                if hasattr(node, 'end_lineno') and node.end_lineno:
                    return node.end_lineno - node.lineno + 1
                return 1
        
        visitor = DebtVisitor()
        visitor.visit(tree)
        issues.extend(visitor.issues)
        
        return issues
    
    def _generate_report(self, issues: List[DebtIssue]) -> DebtReport:
        """ç”Ÿæˆå€ºåŠ¡åˆ†ææŠ¥å‘Š"""
        total_issues = len(issues)
        critical_issues = len([i for i in issues if i.severity == "critical"])
        major_issues = len([i for i in issues if i.severity == "major"])
        minor_issues = len([i for i in issues if i.severity == "minor"])
        
        estimated_total_hours = sum(i.estimated_hours for i in issues)
        
        issues_by_type = defaultdict(int)
        issues_by_file = defaultdict(int)
        
        for issue in issues:
            issues_by_type[issue.type] += 1
            issues_by_file[issue.file_path] += 1
        
        return DebtReport(
            total_issues=total_issues,
            critical_issues=critical_issues,
            major_issues=major_issues,
            minor_issues=minor_issues,
            estimated_total_hours=estimated_total_hours,
            issues_by_type=dict(issues_by_type),
            issues_by_file=dict(issues_by_file),
            detailed_issues=issues
        )
    
    def export_report(self, report: DebtReport, output_path: str) -> None:
        """å¯¼å‡ºæŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report_data = {
            "summary": {
                "total_issues": report.total_issues,
                "critical_issues": report.critical_issues,
                "major_issues": report.major_issues,
                "minor_issues": report.minor_issues,
                "estimated_total_hours": report.estimated_total_hours
            },
            "issues_by_type": report.issues_by_type,
            "issues_by_file": report.issues_by_file,
            "detailed_issues": [
                {
                    "type": issue.type,
                    "severity": issue.severity,
                    "file_path": issue.file_path,
                    "line_number": issue.line_number,
                    "description": issue.description,
                    "suggestion": issue.suggestion,
                    "estimated_hours": issue.estimated_hours
                }
                for issue in report.detailed_issues
            ]
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    def generate_markdown_report(self, report: DebtReport) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        md_content = []
        
        # æ ‡é¢˜å’Œæ‘˜è¦
        md_content.append("# ğŸ” æŠ€æœ¯å€ºåŠ¡åˆ†ææŠ¥å‘Š")
        md_content.append("")
        md_content.append("## ğŸ“Š é—®é¢˜ç»Ÿè®¡")
        md_content.append("")
        md_content.append(f"- **æ€»é—®é¢˜æ•°**: {report.total_issues}")
        md_content.append(f"- **Critical**: {report.critical_issues} ğŸ”´")
        md_content.append(f"- **Major**: {report.major_issues} ğŸŸ¡")
        md_content.append(f"- **Minor**: {report.minor_issues} ğŸŸ¢")
        md_content.append(f"- **é¢„ä¼°ä¿®å¤æ—¶é—´**: {report.estimated_total_hours:.1f} å°æ—¶")
        md_content.append("")
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        md_content.append("## ğŸ·ï¸ é—®é¢˜ç±»å‹åˆ†å¸ƒ")
        md_content.append("")
        for issue_type, count in sorted(report.issues_by_type.items(), key=lambda x: x[1], reverse=True):
            md_content.append(f"- **{issue_type}**: {count}")
        md_content.append("")
        
        # æœ€ä¸¥é‡çš„æ–‡ä»¶
        md_content.append("## ğŸ“ é—®é¢˜æœ€å¤šçš„æ–‡ä»¶")
        md_content.append("")
        top_files = sorted(report.issues_by_file.items(), key=lambda x: x[1], reverse=True)[:10]
        for file_path, count in top_files:
            md_content.append(f"- **{file_path}**: {count} ä¸ªé—®é¢˜")
        md_content.append("")
        
        # è¯¦ç»†é—®é¢˜åˆ—è¡¨ï¼ˆä»…æ˜¾ç¤ºCriticalå’ŒMajorï¼‰
        critical_major_issues = [i for i in report.detailed_issues if i.severity in ["critical", "major"]]
        if critical_major_issues:
            md_content.append("## ğŸš¨ å…³é”®é—®é¢˜è¯¦æƒ…")
            md_content.append("")
            
            for issue in critical_major_issues[:20]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                severity_emoji = "ğŸ”´" if issue.severity == "critical" else "ğŸŸ¡"
                md_content.append(f"### {severity_emoji} {issue.type}")
                md_content.append(f"**æ–‡ä»¶**: {issue.file_path}:{issue.line_number}")
                md_content.append(f"**é—®é¢˜**: {issue.description}")
                md_content.append(f"**å»ºè®®**: {issue.suggestion}")
                md_content.append(f"**é¢„ä¼°æ—¶é—´**: {issue.estimated_hours} å°æ—¶")
                md_content.append("")
        
        return "\n".join(md_content)


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    analyzer = DebtAnalyzer(".")
    report = analyzer.analyze_project()
    
    # å¯¼å‡ºJSONæŠ¥å‘Š
    analyzer.export_report(report, "debt_analysis_report.json")
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_report = analyzer.generate_markdown_report(report)
    with open("debt_analysis_report.md", "w", encoding="utf-8") as f:
        f.write(md_report)
    
    print(f"åˆ†æå®Œæˆï¼å‘ç° {report.total_issues} ä¸ªé—®é¢˜ï¼Œé¢„ä¼°ä¿®å¤æ—¶é—´ {report.estimated_total_hours:.1f} å°æ—¶")